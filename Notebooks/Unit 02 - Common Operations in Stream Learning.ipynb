{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c68a0c",
   "metadata": {},
   "source": [
    "# Moving from standard to stream learning\n",
    "\n",
    "This tutorial is inspired by the [river library](https://riverml.xyz) examples and recipes provided on their website.   Thus, we encourage the student to consult the original documentation for a more in-depth discussion of each of the particular topics.\n",
    "\n",
    "In general,  nearly any approach in machine learning can be summarize in the following steps:\n",
    "\n",
    "1. Defining the problem\n",
    "   1. Identify the kind of problem (supervised vs unsupervised, classification vs regression, etc.)\n",
    "2. Loading the data\n",
    "3. Preprocessing that data\n",
    "    1. Feature extraction\n",
    "    2. Feature selection\n",
    "4. Fitting the model\n",
    "    1. Adjust the hyperparameters\n",
    "5. Evaluate the model according certain measures\n",
    "\n",
    "Before starting, let's examine a classical example that uses all these steps by  **using the scikit-learn library**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db20f47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ROC AUC: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9776</span> <span style=\"font-weight: bold\">(</span>± <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0165</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ROC AUC: \u001b[1;36m0.9776\u001b[0m \u001b[1m(\u001b[0m± \u001b[1;36m0.0165\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from rich import print\n",
    "\n",
    "\n",
    "# Load the data\n",
    "dataset = load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Prepare the pipeline with the preprocessing steps and the model to be used\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),                       # Prepapare the data centered in avg 0 and std 1\n",
    "    ('extractor', PCA(0.95)),                          # Extract features that represents 0.95 of the variability\n",
    "    ('classifier', LogisticRegression(solver='lbfgs')) # Last step the model to perform the classificaction\n",
    "])\n",
    "\n",
    "# Define a determistic cross-validation procedure\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate the performance for each fold of the cross validation\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "scores = cross_val_score(pipe, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Display the average score and it's standard deviation\n",
    "print(f'ROC AUC: {scores.mean():.4f} (± {scores.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c3771",
   "metadata": {},
   "source": [
    "So, what is the problem with this approach?  In reality, it has no problem,  but there are some potential disadvantages, particularly with respect to data size, addition of new data, and heterogeneity of features over time. \n",
    "\n",
    "Dataset  size:   Given the example above, imagine if the data returned by `load_breast_cancer` was too large to fit into the memory of your computer.  In that case,  the program would have crashed. Although some techniques can be applied to minimize this threat, such as optimizing the typing, or the use of sparse data storage,  there is a limit to the optimizations available.   Indeed, if the dataset consisted of millions of samples (corresponding to hundreds of gigabytes), special hardware may have been required just to load the dataset. One solution is to perform out-of-core learning,  which  works on data chunks or mini-batches;  potentially giving rise to other downstream effects related to the data ordering and spurious local minima of each mini-batch when using optimization to obtain model weights.   \n",
    "\n",
    "Growing dataset:   Another potential issue is the incorporation of new data into the model. Traditional approaches require starting from scratch with a new dataset result of the combination of the old data with the new samples available. This is particularly problematic in real-time applications where you have new data available now and then. In many real applications, the solution is to perform a continuous integration pipeline which can deploy a new model nearly several times per minute.\n",
    "\n",
    "Feature heterogeneity:   Finally, another disadvantage with this traditional ML approach is the availability of the features across time and/or datasets from different sources. In particular,  some features might not be available at a particular moment in time, but later become available with a new measurement system.  Another example is in a warehouse.  Imagine that at a particular moment,  the state or amount of an item in stock in the warehouse is not available until a later time (a month later, for example)  when it is included in the dataset.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe261e",
   "metadata": {},
   "source": [
    "# Incremental learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d754d3f",
   "metadata": {},
   "source": [
    "As mentioned before,  incremental learning is also referred to as online learning or stream learning.  The term “online” has been superseded by incremental or stream because of its confusion with vernacular usage of “online learning”, which more often refers to an education option  (this is obvious if you google \"online learning\"). Consequently, the terms \"incremental learning\" and \"stream learning\" are now preferred. \n",
    "\n",
    "The idea of incremental learning is now clear from our previous discussion:  it is concerned with fitting a ML model to a data stream.  Expressed differently,  the data isn't available in its entirety, but rather the observational samples are provided one by one. For example, imagine the previous classical example which, instead of the data set, we have a temporal reference point providing one sample at a time. This can be simulated with a simple loop, such as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1b9d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.760e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.454e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.792e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.810e+02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.263e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.362e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.587e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.884e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.857e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.428e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.548e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.915e+01</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.189e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.660e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.676e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.783e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.456e+00</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.037e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.916e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.686e+02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.996e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.444e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.871e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.039e-02</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m7.760e+00\u001b[0m \u001b[1;36m2.454e+01\u001b[0m \u001b[1;36m4.792e+01\u001b[0m \u001b[1;36m1.810e+02\u001b[0m \u001b[1;36m5.263e-02\u001b[0m \u001b[1;36m4.362e-02\u001b[0m \u001b[1;36m0.000e+00\u001b[0m\n",
       " \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m1.587e-01\u001b[0m \u001b[1;36m5.884e-02\u001b[0m \u001b[1;36m3.857e-01\u001b[0m \u001b[1;36m1.428e+00\u001b[0m \u001b[1;36m2.548e+00\u001b[0m \u001b[1;36m1.915e+01\u001b[0m\n",
       " \u001b[1;36m7.189e-03\u001b[0m \u001b[1;36m4.660e-03\u001b[0m \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m2.676e-02\u001b[0m \u001b[1;36m2.783e-03\u001b[0m \u001b[1;36m9.456e+00\u001b[0m\n",
       " \u001b[1;36m3.037e+01\u001b[0m \u001b[1;36m5.916e+01\u001b[0m \u001b[1;36m2.686e+02\u001b[0m \u001b[1;36m8.996e-02\u001b[0m \u001b[1;36m6.444e-02\u001b[0m \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m0.000e+00\u001b[0m\n",
       " \u001b[1;36m2.871e-01\u001b[0m \u001b[1;36m7.039e-02\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for xi, yi in zip(X, y):\n",
    "    pass\n",
    "\n",
    "print(xi)#we print the last observation to check its format\n",
    "print(yi)#we print the last observation to check its format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a256cb",
   "metadata": {},
   "source": [
    "\n",
    "Since the data is already in memory, this is not the ideal scenario to exemplify it. However, keep in mind that in this particular case we have access to a single sample at a single time (within the loop and represented by the scalar values xi, yi).  In effect, this loop simulates the data stream, where the zip returns a Python iterator.   Note, that this is no different from the way we could iterate through a CSV file, receive a Kafka stream, or obtain the results of an SQL query, row by row.\n",
    "\n",
    "One particular point to be aware of is the fact that in this example `xi` is an instance of `numpy.array` (data were loaded using scikit-learn library). By its design, the library that we are using for streaming learning, `River`, uses the class `dict` as the base of its behavior. The authors of the library assume a point of view where each observation represents a single sample,  which could make sense from the point of view of a stream of data. \n",
    "\n",
    "There are a few considerations that are worth noting:  \n",
    "1. While the use of numpy is not a limitation,  it must be taken into consideration when  high performance (low-latency) is a system requirement.  Remember that `dict` is implemented in **Python**, while the `numpy.array` is implemented at a low level in **C** and **Fortran**. One of the advantages of using `dict` is easy access to the features and a clearer program. \n",
    "\n",
    "2. It must be also noted that  **online processing is different to batch processing, in that vectorization doesn't bring any speedup**. However numeric processing libraries such as numpy are optimized for vectorized operations, thereby introducing considerable overhead for the single sample processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9133930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In numpy.array format:<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.760e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.454e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.792e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.810e+02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.263e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.362e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.587e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.884e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.857e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.428e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.548e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.915e+01</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.189e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.660e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.676e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.783e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.456e+00</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.037e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.916e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.686e+02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.996e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.444e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.871e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.039e-02</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In numpy.array format:\u001b[1m[\u001b[0m\u001b[1;36m7.760e+00\u001b[0m \u001b[1;36m2.454e+01\u001b[0m \u001b[1;36m4.792e+01\u001b[0m \u001b[1;36m1.810e+02\u001b[0m \u001b[1;36m5.263e-02\u001b[0m \u001b[1;36m4.362e-02\u001b[0m \u001b[1;36m0.000e+00\u001b[0m\n",
       " \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m1.587e-01\u001b[0m \u001b[1;36m5.884e-02\u001b[0m \u001b[1;36m3.857e-01\u001b[0m \u001b[1;36m1.428e+00\u001b[0m \u001b[1;36m2.548e+00\u001b[0m \u001b[1;36m1.915e+01\u001b[0m\n",
       " \u001b[1;36m7.189e-03\u001b[0m \u001b[1;36m4.660e-03\u001b[0m \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m2.676e-02\u001b[0m \u001b[1;36m2.783e-03\u001b[0m \u001b[1;36m9.456e+00\u001b[0m\n",
       " \u001b[1;36m3.037e+01\u001b[0m \u001b[1;36m5.916e+01\u001b[0m \u001b[1;36m2.686e+02\u001b[0m \u001b[1;36m8.996e-02\u001b[0m \u001b[1;36m6.444e-02\u001b[0m \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m0.000e+00\u001b[0m\n",
       " \u001b[1;36m2.871e-01\u001b[0m \u001b[1;36m7.039e-02\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In dict format: <span style=\"font-weight: bold\">{</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean radius'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.76</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean texture'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.54</span><span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean perimeter'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47.92</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean area'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">181.0</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean smoothness'</span><span style=\"font-weight: bold\">)</span>: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05263</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean compactness'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04362</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean concavity'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean concave points'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean symmetry'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1587</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fractal dimension'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05884</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'radius error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3857</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'texture error'</span><span style=\"font-weight: bold\">)</span>: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.428</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'perimeter error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.548</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'area error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.15</span><span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'smoothness error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.007189</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'compactness error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00466</span><span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'concavity error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'concave points error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'symmetry </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02676</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'fractal dimension error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002783</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst radius'</span><span style=\"font-weight: bold\">)</span>: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.456</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst texture'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.37</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst perimeter'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59.16</span><span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst area'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">268.6</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst smoothness'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08996</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">compactness'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06444</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst concavity'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst concave points'</span><span style=\"font-weight: bold\">)</span>: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst symmetry'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2871</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst fractal dimension'</span><span style=\"font-weight: bold\">)</span>: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float64</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07039</span><span style=\"font-weight: bold\">)}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In dict format: \u001b[1m{\u001b[0m\u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean radius'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m7.76\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean texture'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m24.54\u001b[0m\u001b[1m)\u001b[0m, \n",
       "\u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean perimeter'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m47.92\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean area'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m181.0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean smoothness'\u001b[0m\u001b[1m)\u001b[0m: \n",
       "\u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.05263\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean compactness'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.04362\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean concavity'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1m)\u001b[0m, \n",
       "\u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean concave points'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean symmetry'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.1587\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean \u001b[0m\n",
       "\u001b[32mfractal dimension'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.05884\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'radius error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.3857\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'texture error'\u001b[0m\u001b[1m)\u001b[0m: \n",
       "\u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1.428\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'perimeter error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2.548\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'area error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m19.15\u001b[0m\u001b[1m)\u001b[0m, \n",
       "\u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'smoothness error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.007189\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'compactness error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.00466\u001b[0m\u001b[1m)\u001b[0m, \n",
       "\u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'concavity error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'concave points error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'symmetry \u001b[0m\n",
       "\u001b[32merror'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.02676\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'fractal dimension error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.002783\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst radius'\u001b[0m\u001b[1m)\u001b[0m: \n",
       "\u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m9.456\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst texture'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m30.37\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst perimeter'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m59.16\u001b[0m\u001b[1m)\u001b[0m, \n",
       "\u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst area'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m268.6\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst smoothness'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.08996\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst \u001b[0m\n",
       "\u001b[32mcompactness'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.06444\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst concavity'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst concave points'\u001b[0m\u001b[1m)\u001b[0m: \n",
       "\u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst symmetry'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.2871\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst fractal dimension'\u001b[0m\u001b[1m)\u001b[0m: \n",
       "\u001b[1;35mnp.float64\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.07039\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"In numpy.array format:{xi}\\n\")\n",
    "print(f\"In dict format: {dict(zip(dataset.feature_names, xi))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6196f",
   "metadata": {},
   "source": [
    "For portability to existing ML algorithms and workflows, `River` provides a convenient wrapper function that transforms the datasets from `scikit-learn`to the required format. For the examples developed in this notebook, you can use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af3f70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean radius'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.76</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean texture'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.54</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean perimeter'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47.92</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean area'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">181.0</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean smoothness'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05263</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean compactness'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04362</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean concavity'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean concave points'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean symmetry'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1587</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean fractal dimension'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05884</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'radius error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3857</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'texture error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.428</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'perimeter error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.548</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'area error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.15</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'smoothness error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.007189</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'compactness error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00466</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'concavity error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'concave points error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'symmetry error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02676</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'fractal dimension error'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002783</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst radius'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.456</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst texture'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.37</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst perimeter'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59.16</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst area'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">268.6</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst smoothness'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08996</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst compactness'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06444</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst concavity'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst concave points'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst symmetry'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2871</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.str_</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'worst fractal dimension'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07039</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean radius'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m7.76\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean texture'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m24.54\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean perimeter'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m47.92\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean area'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m181.0\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean smoothness'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.05263\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean compactness'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.04362\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean concavity'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean concave points'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean symmetry'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.1587\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'mean fractal dimension'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.05884\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'radius error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.3857\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'texture error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m1.428\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'perimeter error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m2.548\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'area error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m19.15\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'smoothness error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.007189\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'compactness error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.00466\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'concavity error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'concave points error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'symmetry error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.02676\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'fractal dimension error'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.002783\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst radius'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m9.456\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst texture'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m30.37\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst perimeter'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m59.16\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst area'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m268.6\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst smoothness'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.08996\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst compactness'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.06444\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst concavity'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst concave points'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst symmetry'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.2871\u001b[0m,\n",
       "    \u001b[1;35mnp.str_\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'worst fractal dimension'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.07039\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from river import stream\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    pass\n",
    "\n",
    "print(xi)#we print the last observation to check its format\n",
    "print(yi)#we print the last observation to check its format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aedc52",
   "metadata": {},
   "source": [
    "While many operations translate quite easily between traditional ML and incremental learning,  some care must be taken when converting between the well-known batches and streams.   We shall illustrate this by considering  the problem of scaling data.   Recall that  the standardization of the data, i.e. scaling the data to have mean 0 and variance 1,  is a trivial operation over batches, while for data streams shall require running or moving statistics.   \n",
    "\n",
    "This simple operation in the batched approach becomes slightly more complicated in the stream data because we don't know the values of the mean and the standard deviation before actually going through all the data. \n",
    "\n",
    "For data streams, a possible approach is to perform the first pass over the data to compute the necessary values and then scale the values during a second pass. However, this potential solution is inconsistent with our objective, and the characteristics of the data:   to process the data once. \n",
    "\n",
    "The solution to this is to use running statistics or moving statistics. The method consists in not using the exact mean and standard deviation,  but rather an estimation that is updated with each new value.   More formally,  given the mean, $\\mu_t$,  and the element count  $n_t$, both at time $t$,  the mean can be easily updated at any moment by applying the following function:\n",
    "$$\\large\n",
    "  n_{t+1} = n_t +1 \\\\   $$\n",
    "$$  \n",
    "  \\large\n",
    "  \\mu_{t+1} = \\mu_t +\\frac{x - \\mu_t}{n_{t+1}}    \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e6560",
   "metadata": {},
   "source": [
    "In the same way, for the variance ($\\sigma$), the previous formula can be completed with:\n",
    "$$\n",
    "\\large\n",
    "  s_{t+1} = s_t + (x-\\mu_t)\\times(x-\\mu_{t+1})\\\\\n",
    "  \\large\n",
    "  \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}}\n",
    "$$\n",
    "\n",
    "where $s_t$  is a running sum of squares and $\\sigma_t$ is the running variance at time $t$. These four formulae can be easily rewritten in Python for example, take the `mean radius` as our Guinea Pig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96b88ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Mean: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.127</span> - Variance: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.397</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Mean: \u001b[1;36m14.127\u001b[0m - Variance: \u001b[1;36m12.397\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's inicialize the variables before any rolling calculation\n",
    "n, mean, s, variance = 0, 0, 0, 0\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    n += 1\n",
    "    mean_t = mean\n",
    "    mean += (xi['mean radius'] - mean_t) / n\n",
    "    s += (xi['mean radius'] - mean_t) * (xi['mean radius'] - mean)\n",
    "    variance = s / n\n",
    "\n",
    "    #print(f'Running mean: {mean:.3f} - Running variance: {variance:.3f}')\n",
    "    \n",
    "print(f'Mean: {mean:.3f} - Variance: {variance:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047a5fd",
   "metadata": {},
   "source": [
    "Now, compare the results with the ones implementations of `numpy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4b5123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> mean: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.127</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m mean: \u001b[1;36m14.127\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> variance: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.397</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m variance: \u001b[1;36m12.397\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "i = list(dataset.feature_names).index('mean radius')\n",
    "print(f'True mean: {np.mean(X[:, i]):.3f}')\n",
    "print(f'True variance: {np.var(X[:, i]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3af056",
   "metadata": {},
   "source": [
    "As expected,  the final results (at time $t$ and with the same $n_t$)  are identical with a key difference that  the `numpy` implementation requires that all data is available for the calculation while in the online case, it is calculated progressively.  Therefore, we should be cognizant of these ideas and realize that the results with only a few instances are not accurate. \n",
    "\n",
    "\n",
    "Although most of the running statistical measurements can be easily developed in Python, the reality is that it is not necessary, since River provides most of them in the `stats` module. For example, to create both the running mean and the running variance we can perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac295804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stats\n",
    "\n",
    "r_mean=stats.Mean()\n",
    "r_variance=stats.Var()\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()): \n",
    "    r_mean.update(xi['mean radius'])\n",
    "    r_variance.update(xi['mean radius'])\n",
    "    #print(f'Running mean: {r_mean.get():.3f} - Running variance: {r_variance.get():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053d916",
   "metadata": {},
   "source": [
    "Thus, once we can calculate the different statistics on the data, the next step is to use those in order to normalize or standardize the data in a similar way as the batched approach. In `River`, several functions have been implemented for these operations and are available in the  `preprocessing` module. For example, to standardize all the features of the previous example we can perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d2928b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled=scaler.transform_one(xi)\n",
    "    #print(f\"xi:{xi}\")\n",
    "    #print(f\"scaled xi {xi_scaled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7ab89",
   "metadata": {},
   "source": [
    "### Applying Machine Learning\n",
    "\n",
    "So, now that the data is scaled,  a machine learning algorithm can be implemented. This example employs linear regression based on *stochastic gradient descent (SGD)*.  The method works by optimizing the internal weights of the model with a loss function that minimizes the variance of the error made between prediction and the true output. In this particular case, the *Squared Error* is chosen as the form of the loss function in order to elude possible compensations due to signal change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3298b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ROC AUC: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9896</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ROC AUC: \u001b[1;36m0.9896\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from river.linear_model import LogisticRegression\n",
    "from river.optim import SGD\n",
    "\n",
    "scaler = StandardScaler()\n",
    "optimizer = SGD(lr=0.01)\n",
    "log_reg = LogisticRegression(optimizer)#SGD is the default one whether the optimizer is not fixed\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer(), shuffle=True, seed=42):\n",
    "\n",
    "    # Scale the features\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled =scaler.transform_one(xi)\n",
    "    # Test the current model on the new \"unobserved\" sample\n",
    "    yi_pred = log_reg.predict_proba_one(xi_scaled)\n",
    "    # Train the model with the new sample\n",
    "    log_reg.learn_one(xi_scaled, yi)\n",
    "\n",
    "    # Store the truth and the prediction\n",
    "    y_true.append(yi)\n",
    "    y_pred.append(yi_pred[True])\n",
    "\n",
    "print(f'ROC AUC: {roc_auc_score(y_true, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c805e6",
   "metadata": {},
   "source": [
    "The results seem to be slightly better than that obtained from  `scikit-learn`. However, let's make a proper comparison.\n",
    "\n",
    "To make an equal comparison, each should use the same CV folds values.   Although we could define the same process quite easily, the two can be made completely comparable by using a built-in `river` module called `compat`.   This module is a wrapper that improves the compatibility with other Python libraries such as sklearn.   This can be done by calling the function  `convert_river_to_sklearn` to obtain an object perfectly compatible with the functions of `scikit-learn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b732fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'y' parameter of cross_val_score must be an array-like or None. Got False instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m convert_river_to_sklearn(model)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Now, we can proceed using the cross_val_score from \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# sklearn with the River wrapped model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Let's compare the results\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROC AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alogon\\anaconda3\\envs\\ML2\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:206\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    204\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 206\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\Alogon\\anaconda3\\envs\\ML2\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:98\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     )\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'y' parameter of cross_val_score must be an array-like or None. Got False instead."
     ]
    }
   ],
   "source": [
    "from river.compose import Pipeline\n",
    "from river.compat import convert_river_to_sklearn\n",
    "\n",
    "\n",
    "# We define a Pipeline, becasuse we need a single object \n",
    "model = Pipeline(\n",
    "    ('scale', StandardScaler()),\n",
    "    ('ml_model', LogisticRegression())\n",
    ")\n",
    "\n",
    "# This funtion returns an object of type SKLRegressorWrapper \n",
    "# which is compatible with the signature of sklearn\n",
    "model = convert_river_to_sklearn(model)\n",
    "\n",
    "# Now, we can proceed using the cross_val_score from \n",
    "# sklearn with the River wrapped model\n",
    "scores = cross_val_score(model, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Let's compare the results\n",
    "print(f'ROC AUC: {scores.mean():.4f} (± {scores.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab62582",
   "metadata": {},
   "source": [
    "Although they are lower than the previous test, the results are comparable to the batch learning approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcecff",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Although briefly introduced in the introduction section, the \"flow\" of information in a machine learning approach can often be encapsulated as a sequence of steps. For this, some standard libraries such as `scikit-learn` or `pandas` provide objects that implement this “declarative” pattern.    River has a special set of modules for implementing pipelines for stream learning. \n",
    "\n",
    "In practice, pipelines are not often used by developers for representing their workflows.  The main reason for this is that most developers come from the world of traditional “batch” oriented ML, where procedural programming is used.   However, in stream learning, pipelines represent a more natural way to map the problem to the programming idiom. \n",
    "\n",
    "In the next example,  we shall compare both the procedural and pipeline-based declarative methods with the same problem.   For this, we shall use a problem from one of the Kaggle competitions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75f1207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Data from the Kaggle Recruit Restaurants challenge.\n",
       "\n",
       "The goal is to predict the number of visitors in each of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">829</span> Japanese restaurants over a priod\n",
       "of roughly <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> weeks. The data is ordered by date and then by restaurant ID.\n",
       "\n",
       "References\n",
       "----------\n",
       "<span style=\"font-weight: bold\">[</span>^<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>: <span style=\"font-weight: bold\">[</span>Recruit Restaurant Visitor Forecasting<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)</span>\n",
       "\n",
       "      Name  Restaurants                                                               \n",
       "      Task  Regression                                                                \n",
       "   Samples  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">252</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108</span>                                                                   \n",
       "  Features  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>                                                                         \n",
       "    Sparse  <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                                                                     \n",
       "      Path  C:\\Users\\Alogon\\river_data\\Restaurants\\kaggle_recruit_restaurants.csv     \n",
       "       URL  <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://maxhalford.github.io/files/datasets/kaggle_recruit_restaurants.zip</span>\n",
       "      Size  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.54</span> MiB                                                                 \n",
       "Downloaded  <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Data from the Kaggle Recruit Restaurants challenge.\n",
       "\n",
       "The goal is to predict the number of visitors in each of \u001b[1;36m829\u001b[0m Japanese restaurants over a priod\n",
       "of roughly \u001b[1;36m16\u001b[0m weeks. The data is ordered by date and then by restaurant ID.\n",
       "\n",
       "References\n",
       "----------\n",
       "\u001b[1m[\u001b[0m^\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m: \u001b[1m[\u001b[0mRecruit Restaurant Visitor Forecasting\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.kaggle.com/c/recruit-restaurant-visitor-forecasting\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "      Name  Restaurants                                                               \n",
       "      Task  Regression                                                                \n",
       "   Samples  \u001b[1;36m252\u001b[0m,\u001b[1;36m108\u001b[0m                                                                   \n",
       "  Features  \u001b[1;36m7\u001b[0m                                                                         \n",
       "    Sparse  \u001b[3;91mFalse\u001b[0m                                                                     \n",
       "      Path  C:\\Users\\Alogon\\river_data\\Restaurants\\kaggle_recruit_restaurants.csv     \n",
       "       URL  \u001b[4;94mhttps://maxhalford.github.io/files/datasets/kaggle_recruit_restaurants.zip\u001b[0m\n",
       "      Size  \u001b[1;36m27.54\u001b[0m MiB                                                                 \n",
       "Downloaded  \u001b[3;91mFalse\u001b[0m                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "from river.datasets import Restaurants\n",
    "\n",
    "data = Restaurants()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf71766",
   "metadata": {},
   "source": [
    "Lets check the structure of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72a9e508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://maxhalford.github.io/files/datasets/kaggle_recruit_restaurants.zip (4.28 MiB)\n",
      "Uncompressing into C:\\Users\\Alogon\\river_data\\Restaurants\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'store_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'air_04341b588bde96cd'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'is_holiday'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'genre_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Izakaya'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'area_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tōkyō-to Nerima-ku Toyotamakita'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'latitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.7356234</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'longitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">139.6516577</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'store_id'\u001b[0m: \u001b[32m'air_04341b588bde96cd'\u001b[0m,\n",
       "        \u001b[32m'date'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2016\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'is_holiday'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'genre_name'\u001b[0m: \u001b[32m'Izakaya'\u001b[0m,\n",
       "        \u001b[32m'area_name'\u001b[0m: \u001b[32m'Tōkyō-to Nerima-ku Toyotamakita'\u001b[0m,\n",
       "        \u001b[32m'latitude'\u001b[0m: \u001b[1;36m35.7356234\u001b[0m,\n",
       "        \u001b[32m'longitude'\u001b[0m: \u001b[1;36m139.6516577\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1;36m10\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(next(iter(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b8c26",
   "metadata": {},
   "source": [
    "###  Procedural Approach: \n",
    "\n",
    "First, lets start by writing the ML analysis code with the procedural approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a71f8440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">MAE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.465114</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "MAE: \u001b[1;36m8.465114\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from river import feature_extraction, linear_model, metrics, preprocessing, stats, utils\n",
    "\n",
    "# Let's create the average of the last 7, 14 and 21 days\n",
    "#TargetAgg: Computes a streaming aggregate of the target values\n",
    "features = (\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\"),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\"),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\")\n",
    ")\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "model = linear_model.LinearRegression()\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in data:\n",
    "\n",
    "    # Derive date features\n",
    "    x['weekday'] = x['date'].weekday() #weekday is a function of datetime.date. It returns a number linked to the week day (monday=0,..., sunday=6)\n",
    "    x['is_weekend'] = x['date'].weekday() in (5, 6)\n",
    "    \n",
    "    # Process the rolling means of the target  \n",
    "    for mean_f in features:\n",
    "    \n",
    "        x = {**x, **mean_f.transform_one(x)}#** unzip a dictionary. In this way, it is concatenating two dicts   \n",
    "        mean_f.learn_one(x, y)\n",
    "   \n",
    "    # Remove the key/value pairs that aren't features\n",
    "    for key in ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']:\n",
    "        x.pop(key)\n",
    "      \n",
    "    # Rescale the data\n",
    "    #This is an example with teaching purpouse. \n",
    "    #In a real example we do not need to scale features such as is_holiday and is weekend \n",
    "    scaler.learn_one(x)\n",
    "    x=scaler.transform_one(x)\n",
    "\n",
    "    # Fit the linear regression\n",
    "    y_pred = model.predict_one(x)\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "    # Update the metric using the out-of-fold prediction\n",
    "    metric.update(y, y_pred)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c238cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_holiday'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.23103573677646685</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'weekday'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0292832579142892</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_weekend'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6249280076334165</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'last_7_mean_mean_by_store_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.350231449980909</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'last_14_mean_mean_by_store_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4125913815779152</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'last_21_mean_mean_by_store_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3980979075298516</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'is_holiday'\u001b[0m: \u001b[1;36m-0.23103573677646685\u001b[0m,\n",
       "    \u001b[32m'weekday'\u001b[0m: \u001b[1;36m1.0292832579142892\u001b[0m,\n",
       "    \u001b[32m'is_weekend'\u001b[0m: \u001b[1;36m1.6249280076334165\u001b[0m,\n",
       "    \u001b[32m'last_7_mean_mean_by_store_id'\u001b[0m: \u001b[1;36m-1.350231449980909\u001b[0m,\n",
       "    \u001b[32m'last_14_mean_mean_by_store_id'\u001b[0m: \u001b[1;36m-1.4125913815779152\u001b[0m,\n",
       "    \u001b[32m'last_21_mean_mean_by_store_id'\u001b[0m: \u001b[1;36m-1.3980979075298516\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check the last sample\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f347fb",
   "metadata": {},
   "source": [
    "###  Declaritive approach using River Pipelines\n",
    "\n",
    "Now, lets rewrite the same code but this time using the declarative approach using River Pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e257728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'store_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'air_fff68b929994bfbd'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_holiday'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'genre_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Bar/Cocktail'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'area_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tōkyō-to Nakano-ku Nakano'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'latitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.7081457</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'longitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">139.66628799999998</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'store_id'\u001b[0m: \u001b[32m'air_fff68b929994bfbd'\u001b[0m,\n",
       "    \u001b[32m'date'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2017\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m22\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'is_holiday'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'genre_name'\u001b[0m: \u001b[32m'Bar/Cocktail'\u001b[0m,\n",
       "    \u001b[32m'area_name'\u001b[0m: \u001b[32m'Tōkyō-to Nakano-ku Nakano'\u001b[0m,\n",
       "    \u001b[32m'latitude'\u001b[0m: \u001b[1;36m35.7081457\u001b[0m,\n",
       "    \u001b[32m'longitude'\u001b[0m: \u001b[1;36m139.66628799999998\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">MAE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.385425</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "MAE: \u001b[1;36m8.385425\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from river import compose\n",
    "\n",
    "# funtion to transform the data in the same features used previously\n",
    "def get_date_features(x):\n",
    "    weekday =  x['date'].weekday()\n",
    "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "\n",
    "# Build the pipeline with the same steps that has been previously done\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(),7),target_name=\"last_7_mean\")),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(),14), target_name=\"last_14_mean\")),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(),21), target_name=\"last_21_mean\"))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in data:\n",
    "\n",
    "    # Make a prediction without using the target\n",
    "    y_pred = model.predict_one(x)\n",
    "\n",
    "    # Update the model using the target\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "    # Update the metric using the out-of-fold prediction\n",
    "    metric.update(y, y_pred)\n",
    "\n",
    "print(x)\n",
    "print(metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1dcb03",
   "metadata": {},
   "source": [
    "As can be seen,  we have arranged all the feature calculations and transformations  into the object `TransformerUnion`, which groups multiple feature processing steps together into a single transformer. This allows for easy combination and management of multiple feature processing steps in a machine learning pipeline. The TransformerUnion object takes a list of transformers as input and applies them in sequence to the input data.\n",
    "\n",
    "Additionally, the `for-loop` is a common operation in an online learning system, and therefore provided as a built-in evaluation function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d86d5f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.385425"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import evaluate\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\" )),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\")),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\"))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE()) #it is replacing the for-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b64cec",
   "metadata": {},
   "source": [
    "Although the code is correct, some additional simplifications can be implemented. Unlike the idiomatic structure of  pipelines in `scikit-learn`, the name of the steps is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c9738f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.385425"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.TransformerUnion(\n",
    "        compose.FuncTransformer(get_date_features),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\"),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\"),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\")\n",
    "    ),\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67392efd",
   "metadata": {},
   "source": [
    "In this case the library will infer one based on the order of the operations provided.\n",
    "\n",
    "The next simplification comes from the fact that the pipeline can be declared with mathematical operations. First, use `+` to declare a `TransformerUnion` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdd6ceca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.385425"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.FuncTransformer(get_date_features) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\") + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\") + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\"),\n",
    "\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f88a0",
   "metadata": {},
   "source": [
    "Likewhise we can use the `|` operator to assemble steps into a `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eed5648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.385425"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (\n",
    "    compose.FuncTransformer(get_date_features) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\") +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\") +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\")\n",
    ")\n",
    "\n",
    "to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "\n",
    "model = model | compose.Discard(*to_discard)#* unzip and use the elements as function arguments\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7746f",
   "metadata": {},
   "source": [
    "One final simplyfication comes from the fact that `river`automatically encapsulates functions in the `FuncTransform` so the final declarative model could be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6042777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20,000] MAE: 8.752951\n",
      "[40,000] MAE: 8.905924\n",
      "[60,000] MAE: 8.928998\n",
      "[80,000] MAE: 8.649261\n",
      "[100,000] MAE: 8.491545\n",
      "[120,000] MAE: 8.422006\n",
      "[140,000] MAE: 8.347251\n",
      "[160,000] MAE: 8.33218\n",
      "[180,000] MAE: 8.44191\n",
      "[200,000] MAE: 8.359173\n",
      "[220,000] MAE: 8.316521\n",
      "[240,000] MAE: 8.369277\n",
      "[252,108] MAE: 8.385425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAE: 8.385425"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "model |= compose.Discard(*to_discard)#* unzip and use the elements as function arguments\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE(), print_every=20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40c1dc",
   "metadata": {},
   "source": [
    "We have included an additional argument to print how the evaluation changes over time. \n",
    "\n",
    "The use of procedural or declarative does not affect the overall performance,  but rather the way we think about the model.  In fact, both models, either procedural or declarative,  will produce the same results and performance.   \n",
    "\n",
    "As final point in the pipelines, it should be mentioned that we can explore graphically the pipeline when it is made based on pipes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f44053b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div class=\"river-component river-pipeline\"><div class=\"river-component river-union\"><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">get_date_features</pre></summary><code class=\"river-estimator-params\">\n",
       "def get_date_features(x):\n",
       "    weekday =  x['date'].weekday()\n",
       "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
       "\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">last_7_mean_mean_by_store_id</pre></summary><code class=\"river-estimator-params\">TargetAgg (\n",
       "  by=['store_id']\n",
       "  how=Rolling (\n",
       "    obj=Mean ()\n",
       "    window_size=7\n",
       "  )\n",
       "  target_name=\"last_7_mean\"\n",
       ")\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">last_14_mean_mean_by_store_id</pre></summary><code class=\"river-estimator-params\">TargetAgg (\n",
       "  by=['store_id']\n",
       "  how=Rolling (\n",
       "    obj=Mean ()\n",
       "    window_size=14\n",
       "  )\n",
       "  target_name=\"last_14_mean\"\n",
       ")\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">last_21_mean_mean_by_store_id</pre></summary><code class=\"river-estimator-params\">TargetAgg (\n",
       "  by=['store_id']\n",
       "  how=Rolling (\n",
       "    obj=Mean ()\n",
       "    window_size=21\n",
       "  )\n",
       "  target_name=\"last_21_mean\"\n",
       ")\n",
       "</code></details></div><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">~['area_name', [...]</pre></summary><code class=\"river-estimator-params\">Discard (\n",
       "  area_name\n",
       "  date\n",
       "  genre_name\n",
       "  latitude\n",
       "  longitude\n",
       "  store_id\n",
       ")\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">StandardScaler</pre></summary><code class=\"river-estimator-params\">StandardScaler (\n",
       "  with_std=True\n",
       ")\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">LinearRegression</pre></summary><code class=\"river-estimator-params\">LinearRegression (\n",
       "  optimizer=SGD (\n",
       "    lr=Constant (\n",
       "      learning_rate=0.01\n",
       "    )\n",
       "  )\n",
       "  loss=Squared ()\n",
       "  l2=0.\n",
       "  l1=0.\n",
       "  intercept_init=0.\n",
       "  intercept_lr=Constant (\n",
       "    learning_rate=0.01\n",
       "  )\n",
       "  clip_gradient=1e+12\n",
       "  initializer=Zeros ()\n",
       ")\n",
       "</code></details></div><style scoped>\n",
       ".river-estimator {\n",
       "    padding: 1em;\n",
       "    border-style: solid;\n",
       "    background: white;\n",
       "    max-width: max-content;\n",
       "}\n",
       "\n",
       ".river-pipeline {\n",
       "    display: flex;\n",
       "    flex-direction: column;\n",
       "    align-items: center;\n",
       "    background: linear-gradient(#000, #000) no-repeat center / 1.5px 100%;\n",
       "}\n",
       "\n",
       ".river-union {\n",
       "    display: flex;\n",
       "    flex-direction: row;\n",
       "    align-items: center;\n",
       "    justify-content: center;\n",
       "    padding: 1em;\n",
       "    border-style: solid;\n",
       "    background: white;\n",
       "}\n",
       "\n",
       ".river-wrapper {\n",
       "    display: flex;\n",
       "    flex-direction: column;\n",
       "    align-items: center;\n",
       "    justify-content: center;\n",
       "    padding: 1em;\n",
       "    border-style: solid;\n",
       "    background: white;\n",
       "}\n",
       "\n",
       ".river-wrapper > .river-estimator {\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "/* Vertical spacing between steps */\n",
       "\n",
       ".river-component + .river-component {\n",
       "    margin-top: 2em;\n",
       "}\n",
       "\n",
       ".river-union > .river-estimator {\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".river-union > .river-component {\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".river-union > .pipeline {\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       "/* Spacing within a union of estimators */\n",
       "\n",
       ".river-union > .river-component + .river-component {\n",
       "    margin-left: 1em;\n",
       "}\n",
       "\n",
       "/* Typography */\n",
       "\n",
       ".river-estimator-params {\n",
       "    display: block;\n",
       "    white-space: pre-wrap;\n",
       "    font-size: 110%;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       ".river-estimator > .river-estimator-params,\n",
       ".river-wrapper > .river-details > river-estimator-params {\n",
       "    background-color: white !important;\n",
       "}\n",
       "\n",
       ".river-wrapper > .river-details {\n",
       "    margin-bottom: 1em;\n",
       "}\n",
       "\n",
       ".river-estimator-name {\n",
       "    display: inline;\n",
       "    margin: 0;\n",
       "    font-size: 110%;\n",
       "}\n",
       "\n",
       "/* Toggle */\n",
       "\n",
       ".river-summary {\n",
       "    display: flex;\n",
       "    align-items:center;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".river-summary > div {\n",
       "    width: 100%;\n",
       "}\n",
       "</style></div>"
      ],
      "text/plain": [
       "Pipeline (\n",
       "  TransformerUnion (\n",
       "    FuncTransformer (\n",
       "      func=\"get_date_features\"\n",
       "    ),\n",
       "    TargetAgg (\n",
       "      by=['store_id']\n",
       "      how=Rolling (\n",
       "        obj=Mean ()\n",
       "        window_size=7\n",
       "      )\n",
       "      target_name=\"last_7_mean\"\n",
       "    ),\n",
       "    TargetAgg (\n",
       "      by=['store_id']\n",
       "      how=Rolling (\n",
       "        obj=Mean ()\n",
       "        window_size=14\n",
       "      )\n",
       "      target_name=\"last_14_mean\"\n",
       "    ),\n",
       "    TargetAgg (\n",
       "      by=['store_id']\n",
       "      how=Rolling (\n",
       "        obj=Mean ()\n",
       "        window_size=21\n",
       "      )\n",
       "      target_name=\"last_21_mean\"\n",
       "    )\n",
       "  ),\n",
       "  Discard (\n",
       "    area_name\n",
       "    date\n",
       "    genre_name\n",
       "    latitude\n",
       "    longitude\n",
       "    store_id\n",
       "  ),\n",
       "  StandardScaler (\n",
       "    with_std=True\n",
       "  ),\n",
       "  LinearRegression (\n",
       "    optimizer=SGD (\n",
       "      lr=Constant (\n",
       "        learning_rate=0.01\n",
       "      )\n",
       "    )\n",
       "    loss=Squared ()\n",
       "    l2=0.\n",
       "    l1=0.\n",
       "    intercept_init=0.\n",
       "    intercept_lr=Constant (\n",
       "      learning_rate=0.01\n",
       "    )\n",
       "    clip_gradient=1e+12\n",
       "    initializer=Zeros ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0354f",
   "metadata": {},
   "source": [
    "Finally,  to debug the behavior of the different steps the function `debug_one`can be used. Imagine that, in this case we train the model with the first 120,000 examples and we want to know what happens with the following one. The next piece of code shows how to use the function `debug_one`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a1ccdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>. Input\n",
       "--------\n",
       "area_name: Tōkyō-to Nerima-ku Toyotamakita <span style=\"font-weight: bold\">(</span>str<span style=\"font-weight: bold\">)</span>\n",
       "date: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">00:00:00</span> <span style=\"font-weight: bold\">(</span>datetime<span style=\"font-weight: bold\">)</span>\n",
       "genre_name: Izakaya <span style=\"font-weight: bold\">(</span>str<span style=\"font-weight: bold\">)</span>\n",
       "is_holiday: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> <span style=\"font-weight: bold\">(</span>bool<span style=\"font-weight: bold\">)</span>\n",
       "latitude: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.73562</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "longitude: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">139.65166</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "store_id: air_04341b588bde96cd <span style=\"font-weight: bold\">(</span>str<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Transformer union\n",
       "--------------------\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span> get_date_features\n",
       "    ---------------------\n",
       "    is_weekend: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> <span style=\"font-weight: bold\">(</span>bool<span style=\"font-weight: bold\">)</span>\n",
       "    weekday: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>int<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1</span> TargetAgg\n",
       "    -------------\n",
       "    last_7_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.28571</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2</span> TargetAgg1\n",
       "    --------------\n",
       "    last_14_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.07143</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3</span> TargetAgg2\n",
       "    --------------\n",
       "    last_21_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.23810</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "is_weekend: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> <span style=\"font-weight: bold\">(</span>bool<span style=\"font-weight: bold\">)</span>\n",
       "last_14_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.07143</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "last_21_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.23810</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "last_7_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.28571</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "weekday: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>int<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. ~<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'area_name'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'genre_name'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'latitude'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'longitude'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'store_id'</span><span style=\"font-weight: bold\">]</span>\n",
       "----------------------------------------------------------------------------\n",
       "is_weekend: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> <span style=\"font-weight: bold\">(</span>bool<span style=\"font-weight: bold\">)</span>\n",
       "last_14_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.07143</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "last_21_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.23810</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "last_7_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.28571</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "weekday: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>int<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. StandardScaler\n",
       "-----------------\n",
       "is_weekend: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.61171</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "last_14_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.30097</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "last_21_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.32644</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "last_7_mean_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.28498</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "weekday: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.51168</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. LinearRegression\n",
       "-------------------\n",
       "Name                            Value      Weight      Contribution  \n",
       "                    Intercept    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.00000</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.99682</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.99682</span>  \n",
       "last_21_mean_mean_by_store_id    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.32644</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.48096</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.57591</span>  \n",
       "last_14_mean_mean_by_store_id    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.30097</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.72577</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.54615</span>  \n",
       "                      weekday    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.51168</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.61055</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.87078</span>  \n",
       " last_7_mean_mean_by_store_id    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.28498</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.23553</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.58763</span>  \n",
       "                   is_weekend   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.61171</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.96213</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.20026</span>  \n",
       "\n",
       "Prediction: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40.77756</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m. Input\n",
       "--------\n",
       "area_name: Tōkyō-to Nerima-ku Toyotamakita \u001b[1m(\u001b[0mstr\u001b[1m)\u001b[0m\n",
       "date: \u001b[1;36m2016\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m00:00:00\u001b[0m \u001b[1m(\u001b[0mdatetime\u001b[1m)\u001b[0m\n",
       "genre_name: Izakaya \u001b[1m(\u001b[0mstr\u001b[1m)\u001b[0m\n",
       "is_holiday: \u001b[3;92mTrue\u001b[0m \u001b[1m(\u001b[0mbool\u001b[1m)\u001b[0m\n",
       "latitude: \u001b[1;36m35.73562\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "longitude: \u001b[1;36m139.65166\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "store_id: air_04341b588bde96cd \u001b[1m(\u001b[0mstr\u001b[1m)\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Transformer union\n",
       "--------------------\n",
       "    \u001b[1;36m1.0\u001b[0m get_date_features\n",
       "    ---------------------\n",
       "    is_weekend: \u001b[3;91mFalse\u001b[0m \u001b[1m(\u001b[0mbool\u001b[1m)\u001b[0m\n",
       "    weekday: \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0mint\u001b[1m)\u001b[0m\n",
       "\n",
       "    \u001b[1;36m1.1\u001b[0m TargetAgg\n",
       "    -------------\n",
       "    last_7_mean_mean_by_store_id: \u001b[1;36m36.28571\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "    \u001b[1;36m1.2\u001b[0m TargetAgg1\n",
       "    --------------\n",
       "    last_14_mean_mean_by_store_id: \u001b[1;36m36.07143\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "    \u001b[1;36m1.3\u001b[0m TargetAgg2\n",
       "    --------------\n",
       "    last_21_mean_mean_by_store_id: \u001b[1;36m36.23810\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "is_weekend: \u001b[3;91mFalse\u001b[0m \u001b[1m(\u001b[0mbool\u001b[1m)\u001b[0m\n",
       "last_14_mean_mean_by_store_id: \u001b[1;36m36.07143\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "last_21_mean_mean_by_store_id: \u001b[1;36m36.23810\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "last_7_mean_mean_by_store_id: \u001b[1;36m36.28571\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "weekday: \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0mint\u001b[1m)\u001b[0m\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. ~\u001b[1m[\u001b[0m\u001b[32m'area_name'\u001b[0m, \u001b[32m'date'\u001b[0m, \u001b[32m'genre_name'\u001b[0m, \u001b[32m'latitude'\u001b[0m, \u001b[32m'longitude'\u001b[0m, \u001b[32m'store_id'\u001b[0m\u001b[1m]\u001b[0m\n",
       "----------------------------------------------------------------------------\n",
       "is_weekend: \u001b[3;91mFalse\u001b[0m \u001b[1m(\u001b[0mbool\u001b[1m)\u001b[0m\n",
       "last_14_mean_mean_by_store_id: \u001b[1;36m36.07143\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "last_21_mean_mean_by_store_id: \u001b[1;36m36.23810\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "last_7_mean_mean_by_store_id: \u001b[1;36m36.28571\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "weekday: \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0mint\u001b[1m)\u001b[0m\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. StandardScaler\n",
       "-----------------\n",
       "is_weekend: \u001b[1;36m-0.61171\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "last_14_mean_mean_by_store_id: \u001b[1;36m1.30097\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "last_21_mean_mean_by_store_id: \u001b[1;36m1.32644\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "last_7_mean_mean_by_store_id: \u001b[1;36m1.28498\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "weekday: \u001b[1;36m0.51168\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. LinearRegression\n",
       "-------------------\n",
       "Name                            Value      Weight      Contribution  \n",
       "                    Intercept    \u001b[1;36m1.00000\u001b[0m    \u001b[1;36m18.99682\u001b[0m       \u001b[1;36m18.99682\u001b[0m  \n",
       "last_21_mean_mean_by_store_id    \u001b[1;36m1.32644\u001b[0m     \u001b[1;36m9.48096\u001b[0m       \u001b[1;36m12.57591\u001b[0m  \n",
       "last_14_mean_mean_by_store_id    \u001b[1;36m1.30097\u001b[0m     \u001b[1;36m2.72577\u001b[0m        \u001b[1;36m3.54615\u001b[0m  \n",
       "                      weekday    \u001b[1;36m0.51168\u001b[0m     \u001b[1;36m5.61055\u001b[0m        \u001b[1;36m2.87078\u001b[0m  \n",
       " last_7_mean_mean_by_store_id    \u001b[1;36m1.28498\u001b[0m     \u001b[1;36m1.23553\u001b[0m        \u001b[1;36m1.58763\u001b[0m  \n",
       "                   is_weekend   \u001b[1;36m-0.61171\u001b[0m    \u001b[1;36m-1.96213\u001b[0m        \u001b[1;36m1.20026\u001b[0m  \n",
       "\n",
       "Prediction: \u001b[1;36m40.77756\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "\n",
    "model |= compose.Discard(*to_discard)#* unzip and use the elements as function arguments\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "for x, y in itertools.islice(data, 120_000):\n",
    "    y_pred = model.predict_one(x)\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "x, y = next(iter(data))\n",
    "print(model.debug_one(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d88b7",
   "metadata": {},
   "source": [
    "# The incremental learning with decision trees\n",
    "\n",
    "Until this point we have focussed on ML techniques that can be implemented in a seamless way with incremental learning, such as the linear or logistic regression. However, there are other techniques that can be adapted to follow this approach, although they require more effort. \n",
    "\n",
    "\n",
    "For example, decision Trees and their variants are an important family of ML techniques.  These methods have proven particularly useful and performant when used in conjunction with ensemble techniques  (bagging, boosting, etc.).  The often cited advantages of decision trees are their flexibility, versatility towards data, and simplicity to implement. \n",
    "\n",
    "Nonetheless,  traditional decision tree algorithms require several iterations over the data, thereby rendering them unsuitable for incremental (stream) learning.   As such,  several candidate tree algorithms have emerged that address this issue and make them suitable for stream learning.  \n",
    "\n",
    "\n",
    "One of the most popular adaptations is Hoeffding Trees (HT) due to their properties similar to batched approaches, such as (C4.5/J48, CART, M5, etc.):\n",
    "* Only require a single pass over the data.\n",
    "* (Theoretical) guarantees the convergence with their batch counterpart with enough observations and a stationary data distribution.\n",
    "* It can operate on scarce resources of memory and computation time.\n",
    "* Some of their variations can deal with non-stationary distributions.\n",
    "\n",
    "\n",
    "In fact,  with the recent spotlight on  Explainable Artificial Intelligence (XAI),  these tree  techniques have taken on a special importance,  because they offer better interpretation.\n",
    "\n",
    "To explore the use of these tree algorithms,  we shall study a simple example where its use can be representative. For this,  we will use the same Restaurant dataset,  downloaded previously, however now analyzed with a single tree. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf02e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 41 s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from river import evaluate, metrics, stats, utils, feature_extraction, preprocessing, compose\n",
    "from river.tree import HoeffdingTreeRegressor\n",
    "import itertools\n",
    "\n",
    "data = Restaurants()\n",
    "to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "def get_date_features(x):\n",
    "    weekday =  x['date'].weekday()\n",
    "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "\n",
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "model |= compose.Discard(*to_discard)\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= HoeffdingTreeRegressor(grace_period=250) #Number of instances a leaf should observe between split attempts\n",
    "\n",
    "for x, y in data:\n",
    "    model.learn_one(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb63e5",
   "metadata": {},
   "source": [
    "From this point,  we can now analyze the resulting model in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18a2b594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">HoeffdingTreeRegressor</pre></summary><code class=\"river-estimator-params\">HoeffdingTreeRegressor (\n",
       "  grace_period=250\n",
       "  max_depth=2980\n",
       "  delta=1e-07\n",
       "  tau=0.05\n",
       "  leaf_prediction=\"adaptive\"\n",
       "  leaf_model=LinearRegression (\n",
       "    optimizer=SGD (\n",
       "      lr=Constant (\n",
       "        learning_rate=0.01\n",
       "      )\n",
       "    )\n",
       "    loss=Squared ()\n",
       "    l2=0.\n",
       "    l1=0.\n",
       "    intercept_init=0.\n",
       "    intercept_lr=Constant (\n",
       "      learning_rate=0.01\n",
       "    )\n",
       "    clip_gradient=1e+12\n",
       "    initializer=Zeros ()\n",
       "  )\n",
       "  model_selector_decay=0.95\n",
       "  nominal_attributes=None\n",
       "  splitter=TEBSTSplitter (\n",
       "    digits=1\n",
       "  )\n",
       "  min_samples_split=5\n",
       "  binary_split=False\n",
       "  max_size=500.\n",
       "  memory_estimate_period=1000000\n",
       "  stop_mem_management=False\n",
       "  remove_poor_attrs=False\n",
       "  merit_preprune=True\n",
       ")\n",
       "</code></details><style scoped>\n",
       ".river-estimator {\n",
       "    padding: 1em;\n",
       "    border-style: solid;\n",
       "    background: white;\n",
       "    max-width: max-content;\n",
       "}\n",
       "\n",
       ".river-pipeline {\n",
       "    display: flex;\n",
       "    flex-direction: column;\n",
       "    align-items: center;\n",
       "    background: linear-gradient(#000, #000) no-repeat center / 1.5px 100%;\n",
       "}\n",
       "\n",
       ".river-union {\n",
       "    display: flex;\n",
       "    flex-direction: row;\n",
       "    align-items: center;\n",
       "    justify-content: center;\n",
       "    padding: 1em;\n",
       "    border-style: solid;\n",
       "    background: white;\n",
       "}\n",
       "\n",
       ".river-wrapper {\n",
       "    display: flex;\n",
       "    flex-direction: column;\n",
       "    align-items: center;\n",
       "    justify-content: center;\n",
       "    padding: 1em;\n",
       "    border-style: solid;\n",
       "    background: white;\n",
       "}\n",
       "\n",
       ".river-wrapper > .river-estimator {\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "/* Vertical spacing between steps */\n",
       "\n",
       ".river-component + .river-component {\n",
       "    margin-top: 2em;\n",
       "}\n",
       "\n",
       ".river-union > .river-estimator {\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".river-union > .river-component {\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".river-union > .pipeline {\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       "/* Spacing within a union of estimators */\n",
       "\n",
       ".river-union > .river-component + .river-component {\n",
       "    margin-left: 1em;\n",
       "}\n",
       "\n",
       "/* Typography */\n",
       "\n",
       ".river-estimator-params {\n",
       "    display: block;\n",
       "    white-space: pre-wrap;\n",
       "    font-size: 110%;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       ".river-estimator > .river-estimator-params,\n",
       ".river-wrapper > .river-details > river-estimator-params {\n",
       "    background-color: white !important;\n",
       "}\n",
       "\n",
       ".river-wrapper > .river-details {\n",
       "    margin-bottom: 1em;\n",
       "}\n",
       "\n",
       ".river-estimator-name {\n",
       "    display: inline;\n",
       "    margin: 0;\n",
       "    font-size: 110%;\n",
       "}\n",
       "\n",
       "/* Toggle */\n",
       "\n",
       ".river-summary {\n",
       "    display: flex;\n",
       "    align-items:center;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".river-summary > div {\n",
       "    width: 100%;\n",
       "}\n",
       "</style></div>"
      ],
      "text/plain": [
       "HoeffdingTreeRegressor (\n",
       "  grace_period=250\n",
       "  max_depth=2980\n",
       "  delta=1e-07\n",
       "  tau=0.05\n",
       "  leaf_prediction=\"adaptive\"\n",
       "  leaf_model=LinearRegression (\n",
       "    optimizer=SGD (\n",
       "      lr=Constant (\n",
       "        learning_rate=0.01\n",
       "      )\n",
       "    )\n",
       "    loss=Squared ()\n",
       "    l2=0.\n",
       "    l1=0.\n",
       "    intercept_init=0.\n",
       "    intercept_lr=Constant (\n",
       "      learning_rate=0.01\n",
       "    )\n",
       "    clip_gradient=1e+12\n",
       "    initializer=Zeros ()\n",
       "  )\n",
       "  model_selector_decay=0.95\n",
       "  nominal_attributes=None\n",
       "  splitter=TEBSTSplitter (\n",
       "    digits=1\n",
       "  )\n",
       "  min_samples_split=5\n",
       "  binary_split=False\n",
       "  max_size=500.\n",
       "  memory_estimate_period=1000000\n",
       "  stop_mem_management=False\n",
       "  remove_poor_attrs=False\n",
       "  merit_preprune=True\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#object analysing\n",
    "model['HoeffdingTreeRegressor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb89c3",
   "metadata": {},
   "source": [
    "That gives an idea of the internal parameters of the object, although we can ask for a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3b4fbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_nodes': 1089,\n",
       " 'n_branches': 544,\n",
       " 'n_leaves': 545,\n",
       " 'n_active_leaves': 545,\n",
       " 'n_inactive_leaves': 0,\n",
       " 'height': 29,\n",
       " 'total_observed_weight': 252108.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary of the tree\n",
    "model['HoeffdingTreeRegressor'].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2902c95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>is_leaf</th>\n",
       "      <th>depth</th>\n",
       "      <th>stats</th>\n",
       "      <th>feature</th>\n",
       "      <th>threshold</th>\n",
       "      <th>splitter</th>\n",
       "      <th>splitters</th>\n",
       "      <th>_disabled_attrs</th>\n",
       "      <th>_last_split_attempt_at</th>\n",
       "      <th>_leaf_model</th>\n",
       "      <th>_model_supports_weights</th>\n",
       "      <th>_fmse_mean</th>\n",
       "      <th>_fmse_model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Var: 327.461181</td>\n",
       "      <td>last_14_mean_mean_by_store_id</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Var: 181.847278</td>\n",
       "      <td>last_14_mean_mean_by_store_id</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Var: 418.16041</td>\n",
       "      <td>last_7_mean_mean_by_store_id</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Var: 58.15876</td>\n",
       "      <td>last_21_mean_mean_by_store_id</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Var: 269.56242</td>\n",
       "      <td>last_21_mean_mean_by_store_id</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>1078</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>Var: 207.17208</td>\n",
       "      <td>last_14_mean_mean_by_store_id</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>1082</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "      <td>Var: 247.713049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEBSTSplitter</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>157.0</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>5132.012782</td>\n",
       "      <td>5241.861165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>1082</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "      <td>Var: 189.247257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEBSTSplitter</td>\n",
       "      <td>{'last_21_mean_mean_by_store_id': TEBSTSplitte...</td>\n",
       "      <td>{}</td>\n",
       "      <td>823.0</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>4145.609909</td>\n",
       "      <td>3916.406135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>1084</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "      <td>Var: 199.647874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEBSTSplitter</td>\n",
       "      <td>{'last_21_mean_mean_by_store_id': TEBSTSplitte...</td>\n",
       "      <td>{}</td>\n",
       "      <td>250.0</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>4619.368926</td>\n",
       "      <td>4745.579406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>1084</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "      <td>Var: 206.14975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEBSTSplitter</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>3712.0</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>2480.455816</td>\n",
       "      <td>2795.180166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1089 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     parent  is_leaf  depth            stats                        feature  \\\n",
       "node                                                                          \n",
       "0      <NA>    False      0  Var: 327.461181  last_14_mean_mean_by_store_id   \n",
       "1         0    False      1  Var: 181.847278  last_14_mean_mean_by_store_id   \n",
       "2         0    False      1   Var: 418.16041   last_7_mean_mean_by_store_id   \n",
       "3         1    False      2    Var: 58.15876  last_21_mean_mean_by_store_id   \n",
       "4         1    False      2   Var: 269.56242  last_21_mean_mean_by_store_id   \n",
       "...     ...      ...    ...              ...                            ...   \n",
       "1084   1078    False     27   Var: 207.17208  last_14_mean_mean_by_store_id   \n",
       "1085   1082     True     28  Var: 247.713049                            NaN   \n",
       "1086   1082     True     28  Var: 189.247257                            NaN   \n",
       "1087   1084     True     28  Var: 199.647874                            NaN   \n",
       "1088   1084     True     28   Var: 206.14975                            NaN   \n",
       "\n",
       "      threshold       splitter  \\\n",
       "node                             \n",
       "0           0.6            NaN   \n",
       "1          -0.3            NaN   \n",
       "2           1.2            NaN   \n",
       "3          -0.8            NaN   \n",
       "4           0.6            NaN   \n",
       "...         ...            ...   \n",
       "1084        0.6            NaN   \n",
       "1085        NaN  TEBSTSplitter   \n",
       "1086        NaN  TEBSTSplitter   \n",
       "1087        NaN  TEBSTSplitter   \n",
       "1088        NaN  TEBSTSplitter   \n",
       "\n",
       "                                              splitters _disabled_attrs  \\\n",
       "node                                                                      \n",
       "0                                                   NaN             NaN   \n",
       "1                                                   NaN             NaN   \n",
       "2                                                   NaN             NaN   \n",
       "3                                                   NaN             NaN   \n",
       "4                                                   NaN             NaN   \n",
       "...                                                 ...             ...   \n",
       "1084                                                NaN             NaN   \n",
       "1085                                                 {}              {}   \n",
       "1086  {'last_21_mean_mean_by_store_id': TEBSTSplitte...              {}   \n",
       "1087  {'last_21_mean_mean_by_store_id': TEBSTSplitte...              {}   \n",
       "1088                                                 {}              {}   \n",
       "\n",
       "      _last_split_attempt_at       _leaf_model _model_supports_weights  \\\n",
       "node                                                                     \n",
       "0                        NaN               NaN                     NaN   \n",
       "1                        NaN               NaN                     NaN   \n",
       "2                        NaN               NaN                     NaN   \n",
       "3                        NaN               NaN                     NaN   \n",
       "4                        NaN               NaN                     NaN   \n",
       "...                      ...               ...                     ...   \n",
       "1084                     NaN               NaN                     NaN   \n",
       "1085                   157.0  LinearRegression                    True   \n",
       "1086                   823.0  LinearRegression                    True   \n",
       "1087                   250.0  LinearRegression                    True   \n",
       "1088                  3712.0  LinearRegression                    True   \n",
       "\n",
       "       _fmse_mean  _fmse_model  \n",
       "node                            \n",
       "0             NaN          NaN  \n",
       "1             NaN          NaN  \n",
       "2             NaN          NaN  \n",
       "3             NaN          NaN  \n",
       "4             NaN          NaN  \n",
       "...           ...          ...  \n",
       "1084          NaN          NaN  \n",
       "1085  5132.012782  5241.861165  \n",
       "1086  4145.609909  3916.406135  \n",
       "1087  4619.368926  4745.579406  \n",
       "1088  2480.455816  2795.180166  \n",
       "\n",
       "[1089 rows x 14 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pandas data frame \n",
    "(model['HoeffdingTreeRegressor']).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39853b71",
   "metadata": {},
   "source": [
    "This last command could be interesting in a classification approach due to the ability to analyze the internal decision of the model and check the thresholds and weights given,   while  in a regression model it is not useful.  Thus,  lets see an example in the context of classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7f499b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'n_nodes'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'n_branches'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'n_leaves'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'n_active_leaves'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'n_inactive_leaves'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'height'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'total_observed_weight'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1250.0</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'n_nodes'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "    \u001b[32m'n_branches'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[32m'n_leaves'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "    \u001b[32m'n_active_leaves'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "    \u001b[32m'n_inactive_leaves'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[32m'height'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "    \u001b[32m'total_observed_weight'\u001b[0m: \u001b[1;36m1250.0\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>is_leaf</th>\n",
       "      <th>depth</th>\n",
       "      <th>stats</th>\n",
       "      <th>feature</th>\n",
       "      <th>threshold</th>\n",
       "      <th>splitter</th>\n",
       "      <th>splitters</th>\n",
       "      <th>_disabled_attrs</th>\n",
       "      <th>_last_split_attempt_at</th>\n",
       "      <th>_mc_correct_weight</th>\n",
       "      <th>_nb_correct_weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{True: 260.0, False: 390.0}</td>\n",
       "      <td>empty_server_form_handler</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>{True: 443.4163997711022, False: 59.8769131081...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GaussianSplitter</td>\n",
       "      <td>{'empty_server_form_handler': GaussianSplitter...</td>\n",
       "      <td>{}</td>\n",
       "      <td>475.293313</td>\n",
       "      <td>249.0</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>{True: 71.58360022889778, False: 404.123086891...</td>\n",
       "      <td>popup_window</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>{False: 31.42653852257483, True: 33.0}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GaussianSplitter</td>\n",
       "      <td>{'empty_server_form_handler': GaussianSplitter...</td>\n",
       "      <td>{}</td>\n",
       "      <td>59.426539</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>{False: 250.57346147742516, True: 6.0}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GaussianSplitter</td>\n",
       "      <td>{'empty_server_form_handler': GaussianSplitter...</td>\n",
       "      <td>{}</td>\n",
       "      <td>241.573461</td>\n",
       "      <td>210.0</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     parent  is_leaf  depth  \\\n",
       "node                          \n",
       "0      <NA>    False      0   \n",
       "1         0     True      1   \n",
       "2         0    False      1   \n",
       "3         2     True      2   \n",
       "4         2     True      2   \n",
       "\n",
       "                                                  stats  \\\n",
       "node                                                      \n",
       "0                           {True: 260.0, False: 390.0}   \n",
       "1     {True: 443.4163997711022, False: 59.8769131081...   \n",
       "2     {True: 71.58360022889778, False: 404.123086891...   \n",
       "3                {False: 31.42653852257483, True: 33.0}   \n",
       "4                {False: 250.57346147742516, True: 6.0}   \n",
       "\n",
       "                        feature  threshold          splitter  \\\n",
       "node                                                           \n",
       "0     empty_server_form_handler   0.545455               NaN   \n",
       "1                           NaN        NaN  GaussianSplitter   \n",
       "2                  popup_window   0.090909               NaN   \n",
       "3                           NaN        NaN  GaussianSplitter   \n",
       "4                           NaN        NaN  GaussianSplitter   \n",
       "\n",
       "                                              splitters _disabled_attrs  \\\n",
       "node                                                                      \n",
       "0                                                   NaN             NaN   \n",
       "1     {'empty_server_form_handler': GaussianSplitter...              {}   \n",
       "2                                                   NaN             NaN   \n",
       "3     {'empty_server_form_handler': GaussianSplitter...              {}   \n",
       "4     {'empty_server_form_handler': GaussianSplitter...              {}   \n",
       "\n",
       "      _last_split_attempt_at  _mc_correct_weight  _nb_correct_weight  \n",
       "node                                                                  \n",
       "0                        NaN                 NaN                 NaN  \n",
       "1                 475.293313               249.0               248.0  \n",
       "2                        NaN                 NaN                 NaN  \n",
       "3                  59.426539                23.0                36.0  \n",
       "4                 241.573461               210.0               196.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from river import evaluate, metrics, stats, utils, feature_extraction, preprocessing, compose\n",
    "from river.tree import HoeffdingTreeClassifier\n",
    "from river.datasets import Phishing #\n",
    "import itertools\n",
    "\n",
    "data = Phishing()\n",
    "\n",
    "model = HoeffdingTreeClassifier(grace_period=50)\n",
    "\n",
    "for x, y in data:\n",
    "    model.learn_one(x, y)\n",
    "    \n",
    "    \n",
    "#summary of the tree\n",
    "print(model.summary)\n",
    "    \n",
    "model.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb384886",
   "metadata": {},
   "source": [
    "As can be seen, the results are more useful than the output of the regression problem.  Also,  a feature that is useful for explainability is the plotting of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50f30a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(process:16912): Pango-WARNING **: 17:50:21.035: couldn't load font \"trebuchet Not-Rotated 11\", falling back to \"Sans Not-Rotated 11\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"307pt\" height=\"230pt\"\n",
       " viewBox=\"0.00 0.00 307.15 230.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 226)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-226 303.15,-226 303.15,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"#fae6d7\" stroke=\"black\" stroke-width=\"1.2\" points=\"191.53,-222 40.62,-222 40.62,-186 191.53,-186 191.53,-222\"/>\n",
       "<text text-anchor=\"middle\" x=\"116.08\" y=\"-200.68\" font-family=\"trebuchet\" font-size=\"11.00\">empty_server_form_handler</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#68b4eb\" stroke=\"black\" stroke-width=\"1.2\" points=\"114.15,-150 0,-150 0,-93 114.15,-93 114.15,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"57.08\" y=\"-139.55\" font-family=\"trebuchet\" font-size=\"11.00\">Class True:</text>\n",
       "<text text-anchor=\"middle\" x=\"57.08\" y=\"-125.3\" font-family=\"trebuchet\" font-size=\"11.00\">\tP(False) = 0.1</text>\n",
       "<text text-anchor=\"middle\" x=\"57.08\" y=\"-111.05\" font-family=\"trebuchet\" font-size=\"11.00\">\tP(True) = 0.9</text>\n",
       "<text text-anchor=\"middle\" x=\"57.08\" y=\"-96.8\" font-family=\"trebuchet\" font-size=\"11.00\">samples: 503</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.6\" d=\"M77.39,-185.69C77.39,-185.69 77.39,-161.36 77.39,-161.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.6\" points=\"80.89,-161.36 77.39,-151.36 73.89,-161.36 80.89,-161.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"64.64\" y=\"-175.88\" font-family=\"Times New Roman,serif\" font-size=\"7.00\">≤ 0.5455</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#eda775\" stroke=\"black\" stroke-width=\"1.2\" points=\"219.65,-139.5 132.5,-139.5 132.5,-103.5 219.65,-103.5 219.65,-139.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"176.07\" y=\"-118.17\" font-family=\"trebuchet\" font-size=\"11.00\">popup_window</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.6\" d=\"M162.01,-185.69C162.01,-185.69 162.01,-150.67 162.01,-150.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.6\" points=\"165.51,-150.67 162.01,-140.67 158.51,-150.67 165.51,-150.67\"/>\n",
       "<text text-anchor=\"middle\" x=\"149.26\" y=\"-170.53\" font-family=\"Times New Roman,serif\" font-size=\"7.00\">&gt; 0.5455</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#fafdfe\" stroke=\"black\" stroke-width=\"1.2\" points=\"167.15,-57 53,-57 53,0 167.15,0 167.15,-57\"/>\n",
       "<text text-anchor=\"middle\" x=\"110.08\" y=\"-46.55\" font-family=\"trebuchet\" font-size=\"11.00\">Class True:</text>\n",
       "<text text-anchor=\"middle\" x=\"110.08\" y=\"-32.3\" font-family=\"trebuchet\" font-size=\"11.00\">\tP(False) = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"110.08\" y=\"-18.05\" font-family=\"trebuchet\" font-size=\"11.00\">\tP(True) = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"110.08\" y=\"-3.8\" font-family=\"trebuchet\" font-size=\"11.00\">samples: 64</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.6\" d=\"M149.82,-103.13C149.82,-103.13 149.82,-68.36 149.82,-68.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.6\" points=\"153.33,-68.36 149.83,-58.36 146.33,-68.36 153.33,-68.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"137.07\" y=\"-88.09\" font-family=\"Times New Roman,serif\" font-size=\"7.00\">≤ 0.0909</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#e68742\" stroke=\"black\" stroke-width=\"1.2\" points=\"299.15,-57 185,-57 185,0 299.15,0 299.15,-57\"/>\n",
       "<text text-anchor=\"middle\" x=\"242.07\" y=\"-46.55\" font-family=\"trebuchet\" font-size=\"11.00\">Class False:</text>\n",
       "<text text-anchor=\"middle\" x=\"242.07\" y=\"-32.3\" font-family=\"trebuchet\" font-size=\"11.00\">\tP(False) = 1.0</text>\n",
       "<text text-anchor=\"middle\" x=\"242.07\" y=\"-18.05\" font-family=\"trebuchet\" font-size=\"11.00\">\tP(True) = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"242.07\" y=\"-3.8\" font-family=\"trebuchet\" font-size=\"11.00\">samples: 256</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.6\" d=\"M202.32,-103.13C202.32,-103.13 202.32,-68.36 202.32,-68.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.6\" points=\"205.83,-68.36 202.33,-58.36 198.83,-68.36 205.83,-68.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.57\" y=\"-88.09\" font-family=\"Times New Roman,serif\" font-size=\"7.00\">&gt; 0.0909</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1e6f0d06cf0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot like a tree\n",
    "#be aware to have graphviz installed on the system\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3591a7",
   "metadata": {},
   "source": [
    "`River`has many implementations of the tree but as the authors of the library remember on their [page](https://riverml.xyz/0.14.0/recipes/on-hoeffding-trees/#1-trees-trees-everywhere-gardening-101-with-river), most of them can be organized in the following table.\n",
    "\n",
    "| Name | Acronym | Task | Non-stationary data? | Comments | Source |\n",
    "| :- | :-: | :- | :-: | :- | :-: |\n",
    "| Hoeffding Tree Classifier | HTC | Classification | No | Basic HT for classification tasks | [[1]](https://dl.acm.org/doi/pdf/10.1145/347090.347107)|\n",
    "| Hoeffding Adaptive Tree Classifier | HATC | Classification | Yes | Modifies HTC by adding an instance of ADWIN to each node to detect and react to drift detection | [[2]](https://link.springer.com/chapter/10.1007/978-3-642-03915-7_22)|\n",
    "| Extremely Fast Decision Tree Classifier | EFDT | Classification | No | Deploys split decisions as soon as possible and periodically revisit decisions and redo them if necessary. Not as fast in practice as the name implies, but it tends to converge faster than HTC to the model generated by a batch DT | [[3]](https://dl.acm.org/doi/abs/10.1145/3219819.3220005)|\n",
    "| Hoeffding Tree Regressor | HTR | Regression | No | Basic HT for regression tasks. It is an adaptation of the [FIRT/FIMT](https://link.springer.com/article/10.1007/s10618-010-0201-y) algorithm that bears some semblance to HTC | [[4]](https://link.springer.com/article/10.1007/s10618-010-0201-y)|\n",
    "| Hoeffding Adaptive Tree Regressor | HATR | Regression | Yes | Modifies HTR by adding an instance of ADWIN to each node to detect and react to drift detection |-|\n",
    "| incremental Structured-Output Prediction Tree Regressor| iSOUPT | Multi-target regression | No | Multi-target version of HTR | [[5]](https://link.springer.com/article/10.1007/s10844-017-0462-7)|\n",
    "| Label Combination Hoeffding Tree Classifier | LCHTC | Multi-label classification | No | Creates a numerical code for each combination of the binary labels and uses HTC to learn from this encoded representation. At prediction time, decodes the modified representation to obtain the original label set |-| \n",
    "\n",
    "\n",
    "As we can see each variant has a very specific target in mind, although they might overlap sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb7374",
   "metadata": {},
   "source": [
    "# Suggested Exercises\n",
    "\n",
    "1. To study incremental learning further, modify the above example that calculates the mean and variance to study data standardization with these methods: \n",
    "* To now calculate the running median and mode of the data. \n",
    "* Compare and contrast the results of the running median and mode to the running mean and variance, and observe how the different measures change as more data is added incrementally. \n",
    "* Explore other ways of normalizing the data available in River.\n",
    "\n",
    "\n",
    "2.  After data normalization,  we provide an example that uses linear regression based on stochastic gradient descent (SGD) and a square loss.   \n",
    "* Write the code that will use another  loss function and optimizer available in River.    \n",
    "* Write the same code, but now wrapping the equivalent Scikit model.  How do the two compare?  \n",
    "\n",
    "\n",
    "\n",
    "3. Trees and Incremental learning:  Modify the example above that uses the HoeffdingTreeRegressor.  To do this, experiment with different values for the grace_period parameter of the HoeffdingTreeRegressor.   The grace_period parameter controls the number of instances a leaf should observe between split attempts. Increasing the value of grace period will lead to fewer splits and thus a smaller tree.  In particular, try different values for grace_period (e.g. 50, 100, 250, 500) and see how it affects the size of the tree and the performance of the model. Write a code snippet that modifies the pipeline and runs the progressive evaluation. Evaluate the model using a evaluation metric of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea035b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate median and mode of the data.\n",
    "r_median=stats.Median()\n",
    "r_mode=stats.Mode()\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()): \n",
    "    r_median.update(xi['mean radius'])\n",
    "    r_mode.update(xi['mean radius'])\n",
    "    #print(f'Running median: {r_meadian.get():.3f} - Running variance: {r_mode.get():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "816bf08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20,000] MAE: 8.752951\n",
      "[40,000] MAE: 8.905924\n",
      "[60,000] MAE: 8.928998\n",
      "[80,000] MAE: 8.649261\n",
      "[100,000] MAE: 8.491545\n",
      "[120,000] MAE: 8.422006\n",
      "[140,000] MAE: 8.347251\n",
      "[160,000] MAE: 8.33218\n",
      "[180,000] MAE: 8.44191\n",
      "[200,000] MAE: 8.359173\n",
      "[220,000] MAE: 8.316521\n",
      "[240,000] MAE: 8.369277\n",
      "[252,108] MAE: 8.385425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAE: 8.385425"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Explore ways of normalizing in river\n",
    "# 3. Use other type of linear regression apart from the one with SGD and square loss.\n",
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "model |= compose.Discard(*to_discard)#* unzip and use the elements as function arguments\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE(), print_every=20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with scikit-learn implementation\n",
    "from river import evaluate\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\" )),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\")),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\"))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE()) #it is replacing the for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05e6cc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>: MAE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.231699</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">250</span>: MAE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.232668</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span>: MAE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.231681</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[1;36m100\u001b[0m: MAE: \u001b[1;36m8.231699\u001b[0m, \u001b[1;36m250\u001b[0m: MAE: \u001b[1;36m8.232668\u001b[0m, \u001b[1;36m500\u001b[0m: MAE: \u001b[1;36m8.231681\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a code snippet that checks several grace periods for a Hoefding tree.\n",
    "\n",
    "from river import evaluate, metrics, stats, utils, feature_extraction, preprocessing, compose\n",
    "from river.tree import HoeffdingTreeRegressor\n",
    "\n",
    "def evaluate_grace_periods(grace_periods, data):\n",
    "    results = {}\n",
    "    \n",
    "    to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "    \n",
    "    def get_date_features(x):\n",
    "        weekday = x['date'].weekday()\n",
    "        return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "    \n",
    "    for gp in grace_periods:\n",
    "        model = get_date_features\n",
    "        \n",
    "        for n in [7, 14, 21]:\n",
    "            model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n), target_name=f\"last_{n}_mean\")\n",
    "        \n",
    "        model |= compose.Discard(*to_discard)\n",
    "        model |= preprocessing.StandardScaler()\n",
    "        model |= HoeffdingTreeRegressor(grace_period=gp)\n",
    "        \n",
    "        mae = evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())\n",
    "        results[gp] = mae\n",
    "    \n",
    "    return results\n",
    "\n",
    "grace_periods = [100, 250, 500]\n",
    "metrics_results = evaluate_grace_periods(grace_periods, Restaurants())\n",
    "print(metrics_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
